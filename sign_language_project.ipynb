{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac512871-b5d7-4e98-b86a-267f0baaad16",
   "metadata": {},
   "source": [
    "1. Import and install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f3bd08-a8a1-4354-ac62-dd391cfb263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981de4fa-6a01-4ce7-9061-5c9b0cc8855d",
   "metadata": {},
   "source": [
    "2. Key points using MP Holistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73938cf7-f12a-4f41-a59a-995f73efbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic    #Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils  #Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d0ccf0-efe1-4a54-9b2c-db399dfa8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  #Color conversion BGR to RGB\n",
    "    image.flags.writeable = False                    #image is no longer writeable\n",
    "    results = model.process(image)                  #Make prediction \n",
    "    image.flags.writeable = True                     # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  #Color Conversion RGB to BGR\n",
    "    return image, results \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaf8784-0253-4118-8dc0-ac8cf4dfce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)  #Draw Face connection\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)  #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)  #Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)  #Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06792870-e3c6-44cc-a8c7-d3689c53ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_style_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1, circle_radius = 1),\n",
    "                             mp_drawing.DrawingSpec(color = (80,110,10), thickness = 1, circle_radius = 1)\n",
    "                             )  \n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7b5445-959b-44b5-b5e9-b0c03a89d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) #this is capturing and accessing camera system\n",
    "with mp_holistic .Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "       \n",
    "        #Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #Make detection \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw Landmarks \n",
    "        draw_style_landmarks(image, results)\n",
    "        #Show to user\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "        #Break out the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4553125-c915-41c8-8c1f-ccde180e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_style_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f28a14-4b8e-4ec8-88a7-ac5aab3767e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2348edb-f9a6-40e5-aa40-a4f80d0f5365",
   "metadata": {},
   "source": [
    "3. Extract key point values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212714b-6881-48b8-ab3f-9ce65427e867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290c32b-d189-493a-88d4-34d5155dadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x,res.y,res.z,res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaaba7-a9f3-48dc-9312-fac464d56b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "\n",
    "lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3) \n",
    "\n",
    "rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten()  if results.right_hand_landmarks else np.zeros(21*3) \n",
    "\n",
    "face = np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce240dc-2e99-4e16-9986-7e0d063ec4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666dd3c0-b368-4314-a010-3ef966b4b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten()  if results.right_hand_landmarks else np.zero(21*3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe18b1e-b32b-4aa3-8421-728b65f8ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "\n",
    "    lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3) \n",
    "\n",
    "    rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten()  if results.right_hand_landmarks else np.zeros(21*3) \n",
    "\n",
    "    face = np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3) \n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147c049-e929-4628-aa0f-d00bd1238a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2069cc-d30e-450e-b6b9-837bee24fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a5754-bd1f-4462-b3fe-2d41ae4d5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86657e3-e6fb-4579-8bae-f7ae8888a602",
   "metadata": {},
   "source": [
    "4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf1cb2-d738-42cc-9073-6618f869ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path fot exported data, numpy arrays\n",
    "DATA_PATH = os.path.join(\"C:\", \"Users\", \"13475\", \"Downloads\", \"MP_Data\")\n",
    "#Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "#Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "#Videos are going to be 30 frames in length\n",
    "sequence_length = 30 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e569e-4f18-4593-a4c0-af3dd2f4c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH,action))).astype(int))\n",
    "    for sequence in range(1, no_sequences+1):\n",
    "       try:\n",
    "           os.makedirs(os.path.join(DATA_PATH, action, str(dirmax+sequence)))\n",
    "       except:\n",
    "               pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68cbde-c130-4626-a0a6-3cb931b8e7fd",
   "metadata": {},
   "source": [
    "5. Collect keypoints values for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e926e3c-9a7b-46fe-80b4-02d87c6ad74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #this is capturing and accessing camera system\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #NEW lOOP\n",
    "    #Loop through actions \n",
    "    for action in actions:\n",
    "        #loop through sequences aka video\n",
    "        for sequence in range(no_sequences):\n",
    "            #Loop through video length aka sequences length \n",
    "            for frame_num in range(sequence_length):\n",
    "       \n",
    "                #Read feed\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                #Make detection \n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "        \n",
    "                #Draw Landmarks \n",
    "                draw_style_landmarks(image, results)\n",
    "\n",
    "                #NEW Apply wait logic \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image,'Starting collection', (120,200),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image,'Collecting frames for {} video numbers {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image,'Collecting frames for {} video numbers {}'.format(action,sequence), (15,12),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                     #Show to user\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                #NEW export keypoints \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)   \n",
    "               \n",
    "            \n",
    "                #Break out the loop\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc9194-613a-4168-b12f-b50dc895649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8e3d4-b1e1-47c1-8e53-472656b0ec31",
   "metadata": {},
   "source": [
    "6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548ef13-f50c-4a49-a6f5-1962abff42fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20f798-e48c-4f5b-81e0-fe7f10bce8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db379c-bdc2-4dea-acb9-f0c87c7b9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81e4b6-7f7c-41aa-97d8-052a2687d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "           res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "           window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d35b6-b8e0-479d-8d9d-863eebab3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\13475\\Downloads\\MP_Data\"\n",
    "\n",
    "sequences, labels = [], []\n",
    "label_map = {label: num for num, label in enumerate(actions)}  # Assuming 'actions' is defined\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    if not os.path.exists(action_path):\n",
    "        print(f\"Directory does not exist: {action_path}\")\n",
    "        continue  # Skip this iteration if the action directory doesn't exist\n",
    "\n",
    "    for sequence_str in os.listdir(action_path):\n",
    "        # Skip non-integer directories (e.g., .ipynb_checkpoints)\n",
    "        if not sequence_str.isdigit():\n",
    "            print(f\"Skipping non-integer directory name: {sequence_str}\")\n",
    "            continue\n",
    "\n",
    "        sequence_path = os.path.join(action_path, sequence_str)\n",
    "        window = []\n",
    "\n",
    "        for frame_num in range(sequence_length):\n",
    "            frame_path = os.path.join(sequence_path, f\"{frame_num}.npy\")\n",
    "            if not os.path.exists(frame_path):\n",
    "                print(f\"File does not exist: {frame_path}\")\n",
    "                break  # Skip this frame if the file doesn't exist\n",
    "\n",
    "            res = np.load(frame_path)\n",
    "            window.append(res)\n",
    "\n",
    "        if len(window) == sequence_length:\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "        else:\n",
    "            print(f\"Incomplete data for sequence: {sequence_path}, expected {sequence_length} frames, got {len(window)}\")\n",
    "\n",
    "sequences = np.array(sequences, dtype=object)  # Use dtype=object for variable-length sequences\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df657287-f4dd-4db7-9773-faeaaf8a6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb5dfc-ebf0-42b5-8b66-9f86663e2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4a68c-a334-4e1a-8bfb-7f492e215c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406cf2a-0012-40f6-bbbd-0f0a2f2732c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a8f1e4-d48d-4e62-b091-51a20c54a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f1232-60ff-4797-89ad-cde453e97c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4972442-0d56-49ca-bb0b-e53e620f810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d0e8b-82ee-4ffa-af20-4a3e7df8a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b2cef-e214-4350-ab16-b9248142b3dd",
   "metadata": {},
   "source": [
    "7.Build and Train LSTM Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9303b4-50c4-4eb3-94bb-116e06417b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60815b61-8672-46cf-ae19-6de7522a2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ac619-7dfb-48d7-adf7-1514d427332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences = True, activation = 'relu', input_shape = (30,1662)))\n",
    "model.add(LSTM(128, return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(64, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a71d5-607c-4aa1-b023-5eca80f6a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7,0.2,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ae76d-ee18-4a04-a17b-427f896a6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8d77e-c1e8-4bac-b572-0a72e7b6834d",
   "metadata": {},
   "source": [
    "Reason why we used Mediapipe holistics and LSTM models rather than state of the art models that uses number of CNN layers followed by a number of LSTM.\n",
    "1. Less data required to create a hyperactive model\n",
    "2. Faster to train, denser network\n",
    "3. Faster detection in real time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73529e0-4903-4110-af99-389d0072f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b37d2-a88c-4620-94cf-0670573bd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "# loss function must be equal to categorical cross entropy. We must use this when you have a multi-class classification model \n",
    "# If youre using binary classificationthen we use binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070d58e-ee54-4fed-9cf2-ff8fb4274d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')  # or 'float64' as needed\n",
    "y_train = y_train.astype('float32')  # Adjust this based on your specific requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113879f-3f47-4e52-816b-c2ab322eca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e4a42-1c7a-4c4a-8c68-9cf56025f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e75ef4-bc3c-48a6-9025-d4375eb29eec",
   "metadata": {},
   "source": [
    "8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b427bb-89ee-40ed-b22c-9fd1da722325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060f8a8-3e13-4590-be75-4295236e6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')  # or 'float64' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd493a-a907-49b7-81df-2866490cce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd759b79-ce38-458b-991a-9a264e4e8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "if np.isnan(X_test).any():\n",
    "    print(\"NaN values found\")\n",
    "    X_test = np.nan_to_num(X_test)  # Replace NaN with 0 and Inf with large finite numbers\n",
    "\n",
    "# Check for Inf values\n",
    "if np.isinf(X_test).any():\n",
    "    print(\"Inf values found\")\n",
    "    X_test = np.nan_to_num(X_test)  # Replace Inf with large finite numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6f56-eda4-436e-a5de-8f2747140875",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72db82-c56f-4a9f-bed6-943e42111f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21dac7-b6d8-4833-9baf-b04a4cb8a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82908ccc-5ca7-44ff-bded-a146926ee1f5",
   "metadata": {},
   "source": [
    "9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed822a0-7baa-4e22-8c13-abfb34ee2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220dbf6-8efb-4a8f-9162-80265240bd0f",
   "metadata": {},
   "source": [
    "9. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368de95-3612-4ecc-872a-35063b404014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf87b7e-01cf-441c-8848-4952af47bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab7d6a-00f2-4c7a-82c9-de1d1ca10cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73179865-9f5d-4ea6-bf9c-56c4fc10ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7646307-1718-4179-8fbc-058e72f2baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca8896-0c57-4975-9103-bc5084188624",
   "metadata": {},
   "source": [
    "11. Test in Real Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ea79a-01c4-4d93-9bcb-60a349da2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. New Detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.4\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0) #this is capturing and accessing camera system\n",
    "with mp_holistic .Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "       \n",
    "        #Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        #Make detection \n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw Landmarks \n",
    "        draw_style_landmarks(image, results)\n",
    "\n",
    "        #2. Prediction Logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[:30]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.prediction(np.expand_dims(sequence, axis = 0))[0]\n",
    "            print(res)\n",
    "\n",
    "        \n",
    "        #Show to user\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "        #Break out the loop\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
